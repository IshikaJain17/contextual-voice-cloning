# -*- coding: utf-8 -*-
"""Sesame-CSM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14aScVypFKVwGYnSRdxrFESrzbSRFsi7k

# CSM (Conversational Speech Model) -
This code implements Sesame AI's CSM-1B, a conversational speech synthesis model that generates natural-sounding speech with semantic understanding.
ðŸŽ¯ Core Concept
CSM doesn't just "read text" - it generates contextually-aware conversational speech by:

Understanding the semantic meaning of text
Generating appropriate prosody (rhythm, stress, intonation)
Maintaining conversational flow with context awareness

## 1. Environment Setup
"""

!git clone https://github.com/SesameAILabs/csm.git

!pip install torchaudio==2.9.1

import sys
import os

# Add the subdirectory to sys.path
sys.path.append(os.path.abspath("/content/csm"))

!pip install -r csm/requirements.txt

!pip install --upgrade torch torchtune torchao torchaudio==2.9.1

!export NO_TORCH_COMPILE=1

from huggingface_hub import login
from google.colab import userdata

# Retrieve the Hugging Face token from Colab's secrets manager
HF_TOKEN = userdata.get("HF_TOKEN")

login(token=HF_TOKEN)

"""## 2. Model Loading"""

import sys
import os

# Add the subdirectory to sys.path
sys.path.append(os.path.abspath("/content/csm"))
print(sys.path)
import torchaudio
import torch

from generator import load_csm_1b

# First, check your current versions
import torch
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")

import os

# Check if the csm directory exists and list its contents
print(f"Contents of /content/csm: {os.listdir('/content/csm')}")

# Attempt to import from generator again
try:
    from generator import load_csm_1b
    print("Successfully imported load_csm_1b from generator.py")
except ModuleNotFoundError:
    print("Error: generator.py or load_csm_1b not found in the path.")
except Exception as e:
    print(f"An error occurred during import: {e}")

if torch.backends.mps.is_available():
    device = "mps"
elif torch.cuda.is_available():
    device = "cuda"
else:
    device = "cpu"

torch.cuda.get_device_properties(0)

generator = load_csm_1b(device=device)

"""## 3. Basic Text-to-Speech"""

audio = generator.generate(
    text="Yeah, for sure. I think employees really become the face of the company here. If they are presentable, it just adds to that polished look. And when you think about those specific requirements like safety gear...uhhh, it's not just about appearances, is it? It's also about ensuring everyoneâ€™s safety. Practicality and... professionalism, coming together? That's what it is, huh?",
    speaker=0,
    context=[],
    # max_audio_length_ms=10_000,
)

"""#### How it generates semantic responses:

Text â†’ Tokenized â†’ Semantic embeddings

Model predicts acoustic features (not just phonemes)

Considers context from previous turns

Generates prosody patterns that match conversational intent
"""

!pip install torchcodec
torchaudio.save("audio1.wav", audio.unsqueeze(0).cpu(), generator.sample_rate)

!python csm/run_csm.py

"""## CSM (Conversational Speech Model) - Multi-Turn Conversation Generator
======================================================================
This script generates a natural-sounding podcast conversation between two speakers
using Sesame AI's CSM-1B model with context-aware semantic speech synthesis.

##### import
"""

import os
import torch
import torchaudio
from huggingface_hub import hf_hub_download
from generator import load_csm_1b, Segment
from dataclasses import dataclass

"""##### SPEAKER VOICE PROMPTS SETUP
============================================================================

These reference audio files provide the voice characteristics for cloning.
 Each prompt contains:
   - "text": The exact transcript of what's spoken in the audio
   - "audio": Path to the reference WAV file

 The model learns speaking style, tone, and prosody from these samples.
"""

# Default prompts are available at https://hf.co/sesame/csm-1b
prompt_filepath_conversational_a = hf_hub_download(
    repo_id="sesame/csm-1b",
    filename="prompts/conversational_a.wav"
)
prompt_filepath_conversational_b = hf_hub_download(
    repo_id="sesame/csm-1b",
    filename="prompts/conversational_b.wav"
)

SPEAKER_PROMPTS = {
    "conversational_a": {
        "text": (
            "like revising for an exam I'd have to try and like keep up the momentum because I'd "
            "start really early I'd be like okay I'm gonna start revising now and then like "
            "you're revising for ages and then I just like start losing steam I didn't do that "
        ),
        "audio": prompt_filepath_conversational_a
    },
    "conversational_b": {
        "text": (
            "like a super Mario level. Like it's very like high detail. And like, once you get "
            "into the park, it just like, everything looks like a computer game and they have all "
            "these, like, you know, if, if there's like a, you know, like in a Mario game, they "
        ),
        "audio": prompt_filepath_conversational_b
    }
}

# HELPER FUNCTIONS
def get_speaker_id(speaker):
    """
    Convert speaker name to numeric ID for the model.
    Returns:
        int: 0 for person1, 1 for person2
    Note: The model uses 0/1 for internal speaker representation
    """
    if speaker == "person1":
        return 0

    return 1

def load_prompt_audio(audio_path: str, target_sample_rate: int) -> torch.Tensor:
    """
    Load and resample audio to match the model's expected sample rate.

    Args:
        audio_path: Path to the WAV file
        target_sample_rate: Desired sample rate (typically 24000 Hz for CSM)

    Returns:
        torch.Tensor: 1D audio tensor at target sample rate

    Process:
        1. Load audio file (may be stereo or different sample rate)
        2. Convert to mono by squeezing
        3. Resample to target rate if needed
    """
    audio_tensor, sample_rate = torchaudio.load(audio_path)
    audio_tensor = audio_tensor.squeeze(0)
    # Resample is lazy so we can always call it
    audio_tensor = torchaudio.functional.resample(
        audio_tensor, orig_freq=sample_rate, new_freq=target_sample_rate
    )
    return audio_tensor

def prepare_prompt(text: str, speaker: int, audio_path: str, sample_rate: int) -> Segment:
    audio_tensor = load_prompt_audio(audio_path, sample_rate)
    return Segment(text=text, speaker=speaker, audio=audio_tensor)

def main():
    # # Select the best available device, skipping MPS due to float64 limitations
    # if torch.cuda.is_available():
    #     device = "cuda"
    # else:
    #     device = "cpu"
    # print(f"Using device: {device}")

    # # Load model
    # generator = load_csm_1b(device)

    # Prepare prompts
    """
    Generate a multi-turn podcast conversation with context-aware speech synthesis.

    Workflow:
        1. Load speaker prompts (voice references)
        2. Define conversation script
        3. Generate each utterance with context from previous turns
        4. Concatenate all audio segments
        5. Save final conversation to WAV file
    """
    # Convert speaker references into Segment objects for voice cloning
    prompt_a = prepare_prompt(
        SPEAKER_PROMPTS["conversational_a"]["text"],
        0,
        SPEAKER_PROMPTS["conversational_a"]["audio"],
        generator.sample_rate
    )

    prompt_b = prepare_prompt(
        SPEAKER_PROMPTS["conversational_b"]["text"],
        1,
        SPEAKER_PROMPTS["conversational_b"]["audio"],
        generator.sample_rate
    )

    # Generate conversation
    # Example conversation structure
    conversation = [
        {"text": "Hey how are you doing?", "speaker_id": 0},
        {"text": "Pretty good, pretty good. How about you?", "speaker_id": 1},
        {"text": "I'm great! So happy to be speaking with you today.", "speaker_id": 0},
        {"text": "Me too! This is some cool stuff, isn't it?", "speaker_id": 1}
    ]
    # PODCAST CONVERSATION SCRIPT
    # ------------------------------------------------------------------------
    # Turn-by-turn dialogue about workplace policies
    # Each dict contains one exchange between person1 and person2
    # Generate each utterance
    generated_segments = []
    prompt_segments = [prompt_a, prompt_b]
    # Generate conversation
    conversations= [
      {
        "person1": "Welcome to the app's podcast on Professional Policies.",
        "person2": "Yeah, absolutely! Today, we'll be diving into a critical topic."
      },
      {
        "person1": "Exactlyâ€”workplace non-discrimination and harassment policies.",
        "person2": "These are super important in ensuring a fair and respectful work environment."
      },
      {
        "person1": "Right, and it's not just about following a policy. It's about creating a culture that genuinely respects every individual's rights.",
        "person2": "Oh, totally. Everyone deserves a professional atmosphereâ€”one that's free from discrimination or harassment."
      },
      {
        "person1": "And, you know, that involves understanding what these policies actually entail.",
        "person2": "Yeah, it does. Like, what constitutes harassment, or how discrimination impacts work culture?"
      }
    ]

    # ------------------------------------------------------------------------
    # INITIALIZE SEGMENT TRACKING
    # ------------------------------------------------------------------------
    # Store generated segments per speaker for context management
    # Context allows the model to maintain conversational flow and voice consistency

    # Generate each utterance
    speaker0_segments = []
    speaker1_segments = []
    generated_segments = []
Voice cloning and semantic AI response understanding
Sesame-CSM.ipynb
Sesame-CSM.ipynb_

[1]
0s
!git clone https://github.com/SesameAILabs/csm.git
fatal: destination path 'csm' already exists and is not an empty directory.

[2]
4s
!pip install torchaudio==2.9.1
Requirement already satisfied: torchaudio==2.9.1 in /usr/local/lib/python3.12/pastedEEthis code working fine make me understnad everything in this for the purpose cloning my own voice also make me understnad how it do semantic response reather then just reading  in short ine asy for ai enginneerCSM (Conversational Speech Model) - Comprehensive Overview
This code implements Sesame AI's CSM-1B, a conversational speech synthesis model that generates natural-sounding speech with semantic understanding.
ðŸŽ¯ Core Concept
CSM doesn't just "read text" - it generates contextually-aware conversational speech by:

Understanding the semantic meaning of text
Generating appropriate prosody (rhythm, stress, intonation)
Maintaining conversational flow with context awareness


ðŸ“‹ Code Breakdown
1. Environment Setup
python# Clone the CSM repository
!git clone https://github.com/SesameAILabs/csm.git

# Install specific torch versions (critical for compatibility)
!pip install torchaudio==2.9.1
!pip install -r csm/requirements.txt
Key Dependencies:

torch==2.4.0 â†’ torch==2.9.1 (upgraded)
torchaudio - audio processing
torchtune, torchao - model optimization
transformers, huggingface_hub - model loading


2. Model Loading
pythonfrom generator import load_csm_1b

# Loads the 1B parameter conversational model
generator = load_csm_1b(device="cuda")  # or "mps" for Mac, "cpu"
What happens internally:

Downloads CSM-1B weights from HuggingFace (sesame/csm-1b)
Loads transformer-based architecture
Sets up audio codec for speech generation


3. Basic Text-to-Speech
pythonaudio = generator.generate(
    text="Your text here",
    speaker=0,  # Speaker ID (0 or 1)
    context=[], # Previous conversation segments
)

# Save audio
torchaudio.save("output.wav", audio.unsqueeze(0).cpu(), generator.sample_rate)
How it generates semantic responses:

Text â†’ Tokenized â†’ Semantic embeddings
Model predicts acoustic features (not just phonemes)
Considers context from previous turns
Generates prosody patterns that match conversational intent


4. Speaker Prompts (Voice Cloning Setup)
pythonSPEAKER_PROMPTS = {
    "conversational_a": {
        "text": "Reference text that was spoken...",
        "audio": "path/to/conversational_a.wav"
    }
}
Purpose: Provides a reference voice for cloning

Text-audio pair gives the model:

Speaking style
Voice characteristics
Prosodic patterns




5. Multi-Turn Conversation Generation
pythondef prepare_prompt(text, speaker, audio_path, sample_rate):
    audio_tensor = load_prompt_audio(audio_path, sample_rate)
    return Segment(text=text, speaker=speaker, audio=audio_tensor)
The Segment Class:
python@dataclass
class Segment:
    text: str          # Transcript
    speaker: int       # 0 or 1
    audio: torch.Tensor  # Audio waveform
Context Management:

Each utterance is a Segment
Previous segments passed as context=[] list
Model uses context for:

Turn-taking patterns
Emotional continuity
Topic awareness




ðŸŽ¤ How to Clone Your Voice
Step 1: Record Reference Audio
bash# Record 30-60 seconds of natural speech
# Requirements:
- Clear audio (no background noise)
- Conversational tone
- Natural pacing
- Sample rate: 24kHz recommended
Step 2: Prepare Your Prompt
pythonMY_VOICE_PROMPT = {
    "text": "Exact transcript of what you said in the recording",
    "audio": "my_voice_sample.wav"
}

# Convert to Segment
my_prompt = prepare_prompt(
    text=MY_VOICE_PROMPT["text"],
    speaker=0,
    audio_path=MY_VOICE_PROMPT["audio"],
    sample_rate=generator.sample_rate
)
Step 3: Generate with Your Voice
pythonaudio = generator.generate(
    text="New text to synthesize in your voice",
    speaker=0,
    context=[my_prompt],  # Provides voice reference
)
```

---

## ðŸ§  **Semantic vs. Just Reading**

### **Traditional TTS (Just Reading):**
```
Text â†’ Phonemes â†’ Audio
"Hello!" â†’ [h É™ l oÊŠ] â†’ ðŸ”Š
```
- Fixed prosody rules
- No context awareness
- Robotic delivery

### **CSM (Semantic Response):**
```
Text + Context â†’ Language Model â†’ Semantic Embedding â†’
â†’ Acoustic Model â†’ Audio Codec â†’ ðŸ”Š
Key Differences:
FeatureTraditional TTSCSMContextNoneUses conversation historyProsodyRule-basedLearned from dataEmotionManual tagsInferred from semanticsTurn-takingN/AModels conversational flow
Example:
python# Same text, different contexts

# Context: Previous speaker was excited
audio1 = generator.generate(
    "Yeah, I agree!",
    context=[excited_previous_turn]
)  # â†’ Enthusiastic delivery

# Context: Previous speaker was sad
audio2 = generator.generate(
    "Yeah, I agree.",
    context=[sad_previous_turn]
)  # â†’ Empathetic, softer delivery
```

---

## ðŸ”§ **Technical Architecture**
```
Input Text
    â†“
[Text Encoder] (Transformer)
    â†“
[Semantic Tokens] (Discrete representations)
    â†“
[Acoustic Model] (Predicts mel-spectrograms)
    â†“
[Neural Codec] (Converts to audio)
    â†“
Audio Waveform
Why semantic tokens matter:

Capture meaning not just words
Enable context propagation
Allow for prosody variation


ðŸ“Š Memory Management
python# Clear GPU memory between generations
import gc
gc.collect()
torch.cuda.empty_cache()
Why needed:

CSM-1B uses ~4GB GPU RAM
Audio tensors accumulate
Prevents OOM errors


ðŸŽ¯ Key Parameters
pythongenerator.generate(
    text="...",
    speaker=0,              # 0 or 1 (which voice)
    context=[],             # List of previous Segments
    max_audio_length_ms=None,  # Optional length limit
    temperature=1.0,        # Sampling randomness (lower = more deterministic)
)

ðŸš€ Best Practices for Voice Cloning

Recording Quality:

Use a good microphone
Record in a quiet room
Speak naturally (not reading)


Reference Length:

30-60 seconds optimal
Include varied intonation
Avoid monotone speech


Text Alignment:

Transcript must match audio exactly
Include filler words ("um", "uh")
Match punctuation to pauses


Context Usage:

Pass your voice prompt as first context segment
Model will adapt new text to your voice characteristics




ðŸ” Debugging Tips
python# Check model is loaded
print(generator.sample_rate)  # Should be 24000

# Verify audio shape
print(audio.shape)  # torch.Size([num_samples])

# Check GPU usage
print(torch.cuda.memory_allocated() / 1e9, "GB")

ðŸ“ Summary for AI Engineers
CSM is NOT phoneme-based TTS. It's a language-model-driven speech synthesizer that:

Encodes semantics using transformer embeddings
Predicts acoustic features conditioned on context
Generates prosody appropriate to conversational intent
Supports voice cloning via audio prompting (few-shot learning)

The "semantic response" comes from:

Context window including previous utterances
Attention mechanism over conversation history
Learned representations of conversational dynamics

This is similar to how GPT generates text, but for audio with prosodic awareness.Retry!python csm/run_csm.py

import os
import torch
import torchaudio
from huggingface_hub import hf_hub_download
from generator import load_csm_1b, Segment
from dataclasses import dataclass

# Default prompts are available at https://hf.co/sesame/csm-1b
prompt_filepath_conversational_a = hf_hub_downloadpastedEEadd comments in this file alsopython"""
CSM (Conversational Speech Model) - Multi-Turn Conversation Generator
======================================================================
This script generates a natural-sounding podcast conversation between two speakers
using Sesame AI's CSM-1B model with context-aware semantic speech synthesis.
"""

# ============================================================================
# IMPORTS
# ============================================================================

import os
import torch
import torchaudio
from huggingface_hub import hf_hub_download
from generator import load_csm_1b, Segment
from dataclasses import dataclass

# ============================================================================
# SPEAKER VOICE PROMPTS SETUP
# ============================================================================
# These reference audio files provide the voice characteristics for cloning.
# Each prompt contains:
#   - "text": The exact transcript of what's spoken in the audio
#   - "audio": Path to the reference WAV file
#
# The model learns speaking style, tone, and prosody from these samples.
# ============================================================================

# Download pre-trained speaker prompts from HuggingFace Hub
prompt_filepath_conversational_a = hf_hub_download(
    repo_id="sesame/csm-1b",
    filename="prompts/conversational_a.wav"
)
prompt_filepath_conversational_b = hf_hub_download(
    repo_id="sesame/csm-1b",
    filename="prompts/conversational_b.wav"
)

# Define speaker characteristics with text-audio pairs
SPEAKER_PROMPTS = {
    "conversational_a": {
        # Natural conversational speech with fillers ("like", pauses)
        # This helps the model capture authentic speaking patterns
        "text": (
            "like revising for an exam I'd have to try and like keep up the momentum because I'd "
            "start really early I'd be like okay I'm gonna start revising now and then like "
            "you're revising for ages and then I just like start losing steam I didn't do that "
        ),
        "audio": prompt_filepath_conversational_a
    },
    "conversational_b": {
        # Different speaking style - descriptive, energetic
        # Note the natural speech patterns and repetitions
        "text": (
            "like a super Mario level. Like it's very like high detail. And like, once you get "
            "into the park, it just like, everything looks like a computer game and they have all "
            "these, like, you know, if, if there's like a, you know, like in a Mario game, they "
        ),
        "audio": prompt_filepath_conversational_b
    }
}

# ============================================================================
# HELPER FUNCTIONS
# ============================================================================

def get_speaker_id(speaker: str) -> int:
    """
    Convert speaker name to numeric ID for the model.

    Args:
        speaker: String identifier ("person1" or "person2")

    Returns:
        int: 0 for person1, 1 for person2

    Note: The model uses 0/1 for internal speaker representation
    """
    if speaker == "person1":
        return 0
    return 1


def load_prompt_audio(audio_path: str, target_sample_rate: int) -> torch.Tensor:
    """
    Load and resample audio to match the model's expected sample rate.

    Args:
        audio_path: Path to the WAV file
        target_sample_rate: Desired sample rate (typically 24000 Hz for CSM)

    Returns:
        torch.Tensor: 1D audio tensor at target sample rate

    Process:
        1. Load audio file (may be stereo or different sample rate)
        2. Convert to mono by squeezing
        3. Resample to target rate if needed
    """
    audio_tensor, sample_rate = torchaudio.load(audio_path)
    audio_tensor = audio_tensor.squeeze(0)  # Convert stereo to mono

    # Resample is lazy in PyTorch - it's safe to always call
    audio_tensor = torchaudio.functional.resample(
        audio_tensor, orig_freq=sample_rate, new_freq=target_sample_rate
    )
    return audio_tensor


def prepare_prompt(text: str, speaker: int, audio_path: str, sample_rate: int) -> Segment:
    """
    Create a Segment object from a voice prompt for voice cloning.

    Args:
        text: Transcript of the audio
        speaker: Speaker ID (0 or 1)
        audio_path: Path to reference audio file
        sample_rate: Target sample rate

    Returns:
        Segment: Contains text, speaker ID, and audio tensor

    Purpose:
        This Segment will be passed as context to subsequent generations,
        allowing the model to clone the voice characteristics.
    """
    audio_tensor = load_prompt_audio(audio_path, sample_rate)
    return Segment(text=text, speaker=speaker, audio=audio_tensor)

# ============================================================================
# MAIN CONVERSATION GENERATION
# ============================================================================

def main():
    """
    Generate a multi-turn podcast conversation with context-aware speech synthesis.

    Workflow:
        1. Load speaker prompts (voice references)
        2. Define conversation script
        3. Generate each utterance with context from previous turns
        4. Concatenate all audio segments
        5. Save final conversation to WAV file
    """

    # ------------------------------------------------------------------------
    # DEVICE SELECTION (Commented out - assumes generator already loaded)
    # ------------------------------------------------------------------------
    # if torch.cuda.is_available():
    #     device = "cuda"
    # else:
    #     device = "cpu"
    # print(f"Using device: {device}")
    # generator = load_csm_1b(device)

    # ------------------------------------------------------------------------
    # PREPARE VOICE PROMPTS
    # ------------------------------------------------------------------------
    # Convert speaker references into Segment objects for voice cloning

    prompt_a = prepare_prompt(
        SPEAKER_PROMPTS["conversational_a"]["text"],
        0,  # Speaker ID for person1
        SPEAKER_PROMPTS["conversational_a"]["audio"],
        generator.sample_rate  # Assumes generator is already loaded globally
    )

    prompt_b = prepare_prompt(
        SPEAKER_PROMPTS["conversational_b"]["text"],
        1,  # Speaker ID for person2
        SPEAKER_PROMPTS["conversational_b"]["audio"],
        generator.sample_rate
    )

    # ------------------------------------------------------------------------
    # CONVERSATION SCRIPT
    # ------------------------------------------------------------------------
    # Example conversation structure (not used in final version below)
    conversation = [
        {"text": "Hey how are you doing?", "speaker_id": 0},
        {"text": "Pretty good, pretty good. How about you?", "speaker_id": 1},
        {"text": "I'm great! So happy to be speaking with you today.", "speaker_id": 0},
        {"text": "Me too! This is some cool stuff, isn't it?", "speaker_id": 1}
    ]

    # ------------------------------------------------------------------------
    # PODCAST CONVERSATION SCRIPT
    # ------------------------------------------------------------------------
    # Turn-by-turn dialogue about workplace policies
    # Each dict contains one exchange between person1 and person2

    conversations = [
        {
            "person1": "Welcome to the app's podcast on Professional Policies.",
            "person2": "Yeah, absolutely! Today, we'll be diving into a critical topic."
        },
        {
            "person1": "Exactlyâ€”workplace non-discrimination and harassment policies.",
            "person2": "These are super important in ensuring a fair and respectful work environment."
        },
        {
            "person1": "Right, and it's not just about following a policy. It's about creating a culture that genuinely respects every individual's rights.",
            "person2": "Oh, totally. Everyone deserves a professional atmosphereâ€”one that's free from discrimination or harassment."
        },
        {
            "person1": "And, you know, that involves understanding what these policies actually entail.",
            "person2": "Yeah, it does. Like, what constitutes harassment, or how discrimination impacts work culture?"
        }
    ]

    # ------------------------------------------------------------------------
    # INITIALIZE SEGMENT TRACKING
    # ------------------------------------------------------------------------
    # Store generated segments per speaker for context management
    # Context allows the model to maintain conversational flow and voice consistency

    speaker0_segments = []  # Person1's previous utterances
    speaker1_segments = []  # Person2's previous utterances
    generated_segments = [] # All utterances in chronological order

    # ------------------------------------------------------------------------
    # GENERATE CONVERSATION TURN-BY-TURN
    # ------------------------------------------------------------------------


    for utterance in conversations:
      # ====================================================================
        # GENERATE PERSON1 (Speaker 0)
        # ====================================================================
        current_speaker_id = get_speaker_id("person1")
        print(f"Generating: {utterance['person1']}, Speaker: {current_speaker_id}")

        # Generate audio with context
        # Context strategy:
        #   - First utterance: Use voice prompt (prompt_a)
        #   - Subsequent utterances: Use previous utterance by same speaker
        # This maintains voice consistency and conversational continuity
        audio_tensor = generator.generate(
            text=utterance['person1'],
            speaker=current_speaker_id,
            # If speaker0_segments is empty, use prompt_a for voice reference
            # Otherwise, use the last segment from this speaker for context
            context= [speaker0_segments[-1]] if len(speaker0_segments) !=0 else [prompt_a],
        )
        # Store the generated segment
        speaker0_segments.append(Segment(text=utterance['person1'], speaker=get_speaker_id("person1"), audio=audio_tensor))
        generated_segments.append(Segment(text=utterance['person1'], speaker=get_speaker_id("person1"), audio=audio_tensor))

        # ====================================================================
        # GENERATE PERSON2 (Speaker 1)
        # ====================================================================
        current_speaker_id = get_speaker_id("person2")
        print(f"Generating: {utterance['person2']}, Speaker:  {current_speaker_id}")

        # Generate response with context from person2's previous utterances
        audio_tensor = generator.generate(
            text=utterance['person2'],
            speaker=current_speaker_id,
            context= [speaker1_segments[-1]] if len(speaker1_segments) !=0 else [prompt_b],
            # Same context strategy as person1
            )

        # Store the generated segment
        speaker1_segments.append(Segment(text=utterance['person2'], speaker=get_speaker_id("person2"), audio=audio_tensor))
        generated_segments.append(Segment(text=utterance['person2'], speaker=get_speaker_id("person2"), audio=audio_tensor))
    # ------------------------------------------------------------------------
    # CONCATENATE AND SAVE
    # ------------------------------------------------------------------------
    # Combine all audio segments into a single continuous conversation
    # Concatenate all generations
    all_audio = torch.cat([seg.audio for seg in generated_segments], dim=0)

    # Save to WAV file
    # unsqueeze(0) adds channel dimension: [samples] -> [1, samples]
    # cpu() moves tensor from GPU to CPU for saving

    torchaudio.save(
        "full_conversation4.wav",
        all_audio.unsqueeze(0).cpu(),
        generator.sample_rate
    )
    print("Successfully generated full_conversation.wav")

# ============================================================================
# EXECUTE
# ============================================================================
main()

# ============================================================================
# MEMORY CLEANUP
# ============================================================================
# Free up GPU memory after generation
# Important for running multiple generations in sequence

import torch, gc
gc.collect()              # Python garbage collection
torch.cuda.empty_cache()  # Clear PyTorch's GPU cache

# Display generator object (for debugging/inspection in notebooks)
generator