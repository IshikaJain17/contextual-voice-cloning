# -*- coding: utf-8 -*-
"""Sesame-CSM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14aScVypFKVwGYnSRdxrFESrzbSRFsi7k

# CSM (Conversational Speech Model) -
This code implements Sesame AI's CSM-1B, a conversational speech synthesis model that generates natural-sounding speech with semantic understanding.
ðŸŽ¯ Core Concept
CSM doesn't just "read text" - it generates contextually-aware conversational speech by:

Understanding the semantic meaning of text
Generating appropriate prosody (rhythm, stress, intonation)
Maintaining conversational flow with context awareness

## 1. Environment Setup
"""

!git clone https://github.com/SesameAILabs/csm.git

!pip install torchaudio==2.9.1

import sys
import os

# Add the subdirectory to sys.path
sys.path.append(os.path.abspath("/content/csm"))

!pip install -r csm/requirements.txt

!pip install --upgrade torch torchtune torchao torchaudio==2.9.1

!export NO_TORCH_COMPILE=1

from huggingface_hub import login
from google.colab import userdata

# Retrieve the Hugging Face token from Colab's secrets manager
HF_TOKEN = userdata.get("HF_TOKEN")

login(token=HF_TOKEN)

"""## 2. Model Loading"""

import sys
import os

# Add the subdirectory to sys.path
sys.path.append(os.path.abspath("/content/csm"))
print(sys.path)
import torchaudio
import torch

from generator import load_csm_1b

# First, check your current versions
import torch
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")

import os

# Check if the csm directory exists and list its contents
print(f"Contents of /content/csm: {os.listdir('/content/csm')}")

# Attempt to import from generator again
try:
    from generator import load_csm_1b
    print("Successfully imported load_csm_1b from generator.py")
except ModuleNotFoundError:
    print("Error: generator.py or load_csm_1b not found in the path.")
except Exception as e:
    print(f"An error occurred during import: {e}")

if torch.backends.mps.is_available():
    device = "mps"
elif torch.cuda.is_available():
    device = "cuda"
else:
    device = "cpu"

torch.cuda.get_device_properties(0)

generator = load_csm_1b(device=device)

"""## 3. Basic Text-to-Speech"""

audio = generator.generate(
    text="Yeah, for sure. I think employees really become the face of the company here. If they are presentable, it just adds to that polished look. And when you think about those specific requirements like safety gear...uhhh, it's not just about appearances, is it? It's also about ensuring everyoneâ€™s safety. Practicality and... professionalism, coming together? That's what it is, huh?",
    speaker=0,
    context=[],
    # max_audio_length_ms=10_000,
)

"""#### How it generates semantic responses:

Text â†’ Tokenized â†’ Semantic embeddings

Model predicts acoustic features (not just phonemes)

Considers context from previous turns

Generates prosody patterns that match conversational intent
"""

!pip install torchcodec
torchaudio.save("audio1.wav", audio.unsqueeze(0).cpu(), generator.sample_rate)

!python csm/run_csm.py

"""## CSM (Conversational Speech Model) - Multi-Turn Conversation Generator
======================================================================
This script generates a natural-sounding podcast conversation between two speakers
using Sesame AI's CSM-1B model with context-aware semantic speech synthesis.

##### import
"""

import os
import torch
import torchaudio
from huggingface_hub import hf_hub_download
from generator import load_csm_1b, Segment
from dataclasses import dataclass

"""##### SPEAKER VOICE PROMPTS SETUP
============================================================================

These reference audio files provide the voice characteristics for cloning.
 Each prompt contains:
   - "text": The exact transcript of what's spoken in the audio
   - "audio": Path to the reference WAV file

 The model learns speaking style, tone, and prosody from these samples.
"""

# Default prompts are available at https://hf.co/sesame/csm-1b
prompt_filepath_conversational_a = hf_hub_download(
    repo_id="sesame/csm-1b",
    filename="prompts/conversational_a.wav"
)
prompt_filepath_conversational_b = hf_hub_download(
    repo_id="sesame/csm-1b",
    filename="prompts/conversational_b.wav"
)

SPEAKER_PROMPTS = {
    "conversational_a": {
        "text": (
            "like revising for an exam I'd have to try and like keep up the momentum because I'd "
            "start really early I'd be like okay I'm gonna start revising now and then like "
            "you're revising for ages and then I just like start losing steam I didn't do that "
        ),
        "audio": prompt_filepath_conversational_a
    },
    "conversational_b": {
        "text": (
            "like a super Mario level. Like it's very like high detail. And like, once you get "
            "into the park, it just like, everything looks like a computer game and they have all "
            "these, like, you know, if, if there's like a, you know, like in a Mario game, they "
        ),
        "audio": prompt_filepath_conversational_b
    }
}

# HELPER FUNCTIONS
def get_speaker_id(speaker):
    """
    Convert speaker name to numeric ID for the model.
    Returns:
        int: 0 for person1, 1 for person2
    Note: The model uses 0/1 for internal speaker representation
    """
    if speaker == "person1":
        return 0

    return 1

def load_prompt_audio(audio_path: str, target_sample_rate: int) -> torch.Tensor:
    """
    Load and resample audio to match the model's expected sample rate.

    Args:
        audio_path: Path to the WAV file
        target_sample_rate: Desired sample rate (typically 24000 Hz for CSM)

    Returns:
        torch.Tensor: 1D audio tensor at target sample rate

    Process:
        1. Load audio file (may be stereo or different sample rate)
        2. Convert to mono by squeezing
        3. Resample to target rate if needed
    """
    audio_tensor, sample_rate = torchaudio.load(audio_path)
    audio_tensor = audio_tensor.squeeze(0)
    # Resample is lazy so we can always call it
    audio_tensor = torchaudio.functional.resample(
        audio_tensor, orig_freq=sample_rate, new_freq=target_sample_rate
    )
    return audio_tensor

def prepare_prompt(text: str, speaker: int, audio_path: str, sample_rate: int) -> Segment:
    audio_tensor = load_prompt_audio(audio_path, sample_rate)
    return Segment(text=text, speaker=speaker, audio=audio_tensor)

# ============================================================================
# MAIN CONVERSATION GENERATION
# ============================================================================

def main():
    """
    Generate a multi-turn podcast conversation with context-aware speech synthesis.

    Workflow:
        1. Load speaker prompts (voice references)
        2. Define conversation script
        3. Generate each utterance with context from previous turns
        4. Concatenate all audio segments
        5. Save final conversation to WAV file
    """

    # ------------------------------------------------------------------------
    # DEVICE SELECTION (Commented out - assumes generator already loaded)
    # ------------------------------------------------------------------------
    # if torch.cuda.is_available():
    #     device = "cuda"
    # else:
    #     device = "cpu"
    # print(f"Using device: {device}")
    # generator = load_csm_1b(device)

    # ------------------------------------------------------------------------
    # PREPARE VOICE PROMPTS
    # ------------------------------------------------------------------------
    # Convert speaker references into Segment objects for voice cloning

    prompt_a = prepare_prompt(
        SPEAKER_PROMPTS["conversational_a"]["text"],
        0,  # Speaker ID for person1
        SPEAKER_PROMPTS["conversational_a"]["audio"],
        generator.sample_rate  # Assumes generator is already loaded globally
    )

    prompt_b = prepare_prompt(
        SPEAKER_PROMPTS["conversational_b"]["text"],
        1,  # Speaker ID for person2
        SPEAKER_PROMPTS["conversational_b"]["audio"],
        generator.sample_rate
    )

    # ------------------------------------------------------------------------
    # CONVERSATION SCRIPT
    # ------------------------------------------------------------------------
    # Example conversation structure (not used in final version below)
    conversation = [
        {"text": "Hey how are you doing?", "speaker_id": 0},
        {"text": "Pretty good, pretty good. How about you?", "speaker_id": 1},
        {"text": "I'm great! So happy to be speaking with you today.", "speaker_id": 0},
        {"text": "Me too! This is some cool stuff, isn't it?", "speaker_id": 1}
    ]

    # ------------------------------------------------------------------------
    # PODCAST CONVERSATION SCRIPT
    # ------------------------------------------------------------------------
    # Turn-by-turn dialogue about workplace policies
    # Each dict contains one exchange between person1 and person2

    conversations = [
        {
            "person1": "Welcome to the app's podcast on Professional Policies.",
            "person2": "Yeah, absolutely! Today, we'll be diving into a critical topic."
        },
        {
            "person1": "Exactlyâ€”workplace non-discrimination and harassment policies.",
            "person2": "These are super important in ensuring a fair and respectful work environment."
        },
        {
            "person1": "Right, and it's not just about following a policy. It's about creating a culture that genuinely respects every individual's rights.",
            "person2": "Oh, totally. Everyone deserves a professional atmosphereâ€”one that's free from discrimination or harassment."
        },
        {
            "person1": "And, you know, that involves understanding what these policies actually entail.",
            "person2": "Yeah, it does. Like, what constitutes harassment, or how discrimination impacts work culture?"
        }
    ]

    # ------------------------------------------------------------------------
    # INITIALIZE SEGMENT TRACKING
    # ------------------------------------------------------------------------
    # Store generated segments per speaker for context management
    # Context allows the model to maintain conversational flow and voice consistency

    speaker0_segments = []  # Person1's previous utterances
    speaker1_segments = []  # Person2's previous utterances
    generated_segments = [] # All utterances in chronological order

    # ------------------------------------------------------------------------
    # GENERATE CONVERSATION TURN-BY-TURN
    # ------------------------------------------------------------------------


    for utterance in conversations:
      # ====================================================================
        # GENERATE PERSON1 (Speaker 0)
        # ====================================================================
        current_speaker_id = get_speaker_id("person1")
        print(f"Generating: {utterance['person1']}, Speaker: {current_speaker_id}")

        # Generate audio with context
        # Context strategy:
        #   - First utterance: Use voice prompt (prompt_a)
        #   - Subsequent utterances: Use previous utterance by same speaker
        # This maintains voice consistency and conversational continuity
        audio_tensor = generator.generate(
            text=utterance['person1'],
            speaker=current_speaker_id,
            # If speaker0_segments is empty, use prompt_a for voice reference
            # Otherwise, use the last segment from this speaker for context
            context= [speaker0_segments[-1]] if len(speaker0_segments) !=0 else [prompt_a],
        )
        # Store the generated segment
        speaker0_segments.append(Segment(text=utterance['person1'], speaker=get_speaker_id("person1"), audio=audio_tensor))
        generated_segments.append(Segment(text=utterance['person1'], speaker=get_speaker_id("person1"), audio=audio_tensor))

        # ====================================================================
        # GENERATE PERSON2 (Speaker 1)
        # ====================================================================
        current_speaker_id = get_speaker_id("person2")
        print(f"Generating: {utterance['person2']}, Speaker:  {current_speaker_id}")

        # Generate response with context from person2's previous utterances
        audio_tensor = generator.generate(
            text=utterance['person2'],
            speaker=current_speaker_id,
            context= [speaker1_segments[-1]] if len(speaker1_segments) !=0 else [prompt_b],
            # Same context strategy as person1
            )

        # Store the generated segment
        speaker1_segments.append(Segment(text=utterance['person2'], speaker=get_speaker_id("person2"), audio=audio_tensor))
        generated_segments.append(Segment(text=utterance['person2'], speaker=get_speaker_id("person2"), audio=audio_tensor))
    # ------------------------------------------------------------------------
    # CONCATENATE AND SAVE
    # ------------------------------------------------------------------------
    # Combine all audio segments into a single continuous conversation
    # Concatenate all generations
    all_audio = torch.cat([seg.audio for seg in generated_segments], dim=0)

    # Save to WAV file
    # unsqueeze(0) adds channel dimension: [samples] -> [1, samples]
    # cpu() moves tensor from GPU to CPU for saving

    torchaudio.save(
        "full_conversation4.wav",
        all_audio.unsqueeze(0).cpu(),
        generator.sample_rate
    )
    print("Successfully generated full_conversation.wav")

# ============================================================================
# EXECUTE
# ============================================================================
main()

# ============================================================================
# MEMORY CLEANUP
# ============================================================================
# Free up GPU memory after generation
# Important for running multiple generations in sequence

import torch, gc
gc.collect()              # Python garbage collection
torch.cuda.empty_cache()  # Clear PyTorch's GPU cache

# Display generator object (for debugging/inspection in notebooks)
generator